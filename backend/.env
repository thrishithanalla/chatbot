# Ollama Configuration
# Ensure the Ollama server is running and accessible at this URL.
OLLAMA_BASE_URL=http://127.0.0.1:11434/

# --- Choose your LLMs ---
# Make sure the models specified here have been pulled using `ollama pull <model_name>`

# Generation/Analysis Model (Used for chat responses, analysis tasks)
# Recommended: deepseek-coder:6.7b-instruct (Good balance of coding/reasoning)
# Alternative: deepseek-r1 (If provided)
# Alternative: llama3:8b (Strong general model)
# Alternative: mistral:7b (Good general model)
OLLAMA_MODEL=deepseek-r1:1.5b
# OLLAMA_MODEL=llama3:8b
# OLLAMA_MODEL=llama3.2
#OLLAMA_MODEL=mistral:7b-instruct

# OLLAMA_MODEL=qwen2.5:14b-instruct
# Embedding Model (Used for creating vector representations of text)
# Recommended: mxbai-embed-large (Top performer on MTEB leaderboard)
# Alternative: nomic-embed-text (Another good option)
OLLAMA_EMBED_MODEL=mxbai-embed-large
# OLLAMA_EMBED_MODEL=nomic-embed-text

# Optional: Ollama Request Timeout (seconds)
# Increase if you get timeout errors during long embedding or generation tasks
# OLLAMA_REQUEST_TIMEOUT=180

# --- Application Configuration ---
# Paths are relative to the 'backend' directory. Defaults are usually fine.
# FAISS_FOLDER=faiss_store
# UPLOAD_FOLDER=uploads
# DATABASE_NAME=chat_history.db
# DEFAULT_PDFS_FOLDER=default_pdfs

# --- RAG Configuration ---
# RAG_CHUNK_K=5               # Max unique chunks sent to LLM for synthesis
# RAG_SEARCH_K_PER_QUERY=3    # Chunks retrieved per sub-query before deduplication
# MULTI_QUERY_COUNT=3         # Number of sub-queries generated (0 to disable)

# --- Analysis Configuration ---
# ANALYSIS_MAX_CONTEXT_LENGTH=8000 # Max characters of document text sent for analysis

# --- Logging Configuration ---
# Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Set to DEBUG for detailed troubleshooting.
LOGGING_LEVEL=INFO
# LOGGING_LEVEL=DEBUG
GEMINI_API_KEY=AIzaSyBkWfqv2SWWroIVbVegqsi1_5FiLlZcLn0

# --- Whisper STT Configuration ---
# Model size for OpenAI Whisper (e.g., tiny, base, small, medium, large).
# 'base' is a good starting point. Ensure you have enough RAM/VRAM.
# WHISPER_MODEL_SIZE=base